# -*- coding: utf-8 -*-
"""Sentiment Analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16EWz4oa_k53Lf6XYFq_cT_kBBus7K5FP
"""

!pip install vaderSentiment

from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
vs = SentimentIntensityAnalyzer()

text = 'The product is really Awesome'
vs.polarity_scores(text)

text2 = 'Super cheap material. I would not buy even the curtains made from this synthetic. I did not even try it.'
vs.polarity_scores(text2)

text3 = 'Does not look like the dress pictured. Too short to wear as a dress, I’m 5’7” and got a large to be sure that it fit and it barely covers my bum.'
vs.polarity_scores(text3)

# Web scraping

import requests
from bs4 import BeautifulSoup
import pandas as pd
import numpy as np 
import os

url = 'https://inshorts.com/en/read/world'
news_data = []
news_category = url.split('/')[-1]
data = requests.get(url)
soup = BeautifulSoup(data.content)
print(soup)

urls = ['https://inshorts.com/en/read/world',
        'https://inshorts.com/en/read/sports',
        'https://inshorts.com/en/read/politics'
        ]

def build_dataset(urls):
  news_data = []
  for url in urls:
    news_category = url.split('/')[-1]
    data = requests.get(url)
    soup = BeautifulSoup(data.content)

    news_articles = [{'news_headline':headline.find('span', attrs={"itemprop":"headline"}).string,
                      'news_article':article.find('div', attrs={"itemprop":"articleBody"}).string,
                      'news_category':news_category}
                     
                     for headline,article in zip(soup.find_all('div',class_=["news-card-title news-right-box"]),
                                                 soup.find_all('div',class_=["news-card-content news-right-box"]))]
                     
    news_articles = news_articles[0:20]
    news_data.extend(news_articles)

  df = pd.DataFrame(news_data)
  df = df[['news_headline','news_article','news_category']]
  return df

df = build_dataset(urls)
df.tail()

df.to_csv('news.csv', index=False)

import nltk
nltk.download('stopwords')
stopword_list = nltk.corpus.stopwords.words('english')
stopword_list.remove('no')
stopword_list.remove('not')
len(stopword_list)

# Remove html tag
def html_tag(text):
  soup = BeautifulSoup(text, "html.parser")
  new_text = soup.get_text()
  return new_text

html_tag('<html><h2> Some important info </h2></html>')

# Expand Contractions
!pip install contractions
import contractions
def con(text):
  expand = contractions.fix(text)
  return expand

con("Y'all can't expand I'd think")

import re 
def remove_sp(text):
  pattern = r'[^A-Za-z0-9\s]'
  text = re.sub(pattern,'',text)
  return text

remove_sp("Wow this was fun! +- skm")

from nltk.tokenize.toktok import ToktokTokenizer
tokenizer = ToktokTokenizer()

def remove_stopwords(text):
  tokens = tokenizer.tokenize(text)
  tokens = [token.strip() for token in tokens]
  filtered_tokens = [token for token in tokens if token not in stopword_list]
  filtered_text = ' '.join(filtered_tokens)
  return filtered_text

remove_stopwords("The, and, if are all stopwords and even not")

# 1. Lower case
# 2. HTML Tags
# 3. Contractions
# 4. Special Characters
# 5. Stop words

df.news_headline = df.news_headline.apply(lambda x:x.lower())
df.news_article = df.news_article.apply(lambda x:x.lower())

df.news_headline = df.news_headline.apply(html_tag)
df.news_article = df.news_article.apply(html_tag)

df.news_headline = df.news_headline.apply(con)
df.news_article = df.news_article.apply(con)

df.news_headline = df.news_headline.apply(remove_sp)
df.news_article = df.news_article.apply(remove_sp)

df.news_headline = df.news_headline.apply(remove_stopwords)
df.news_article = df.news_article.apply(remove_stopwords)

df.head()

df['compound'] = df['news_article'].apply(lambda x: vs.polarity_scores(x)['compound'])

df

